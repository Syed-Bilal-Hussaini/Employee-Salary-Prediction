import pandas as pd

data=pd.read_csv("/content/adult 3.csv")

data.shape

data

data.tail(7)

data.head(7)

data.isna().sum()

print(data.occupation.value_counts())

data.occupation = data.occupation.replace({"?": "Private"})

data.workclass.value_counts()

data.workclass = data.workclass.replace({"?": "Others"})

data=data[data['workclass']!='Without-pay']
data=data[data['workclass']!='Never-worked']

data.shape

print(data.education.value_counts())

data=data[data['education']!='1st-4th']
data=data[data['education']!='Preschool']
data=data[data['education']!='5th-6th']

print(data.education.value_counts())

data.drop(columns=['education'], inplace=True)
data.drop(columns=['fnlwgt'], inplace=True)

data

import matplotlib.pyplot as plt
plt.boxplot(data['age'])
plt.show


data=data[(data['age']<=75)&(data['age']>=17)]

import matplotlib.pyplot as plt
plt.boxplot(data['age'])
plt.show

#Label encoding
from sklearn.preprocessing import LabelEncoder
categorical_cols = ['workclass', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country']
for col in categorical_cols:
    encoder = LabelEncoder()
    data[col] = encoder.fit_transform(data[col])

x=data.drop(columns=['income'])
y=data['income']

data

y

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
x = scaler.fit_transform(x) # Refit the scaler on the fully preprocessed data
x

from sklearn.model_selection import train_test_split
xtrain, xtest , ytrain, ytest= train_test_split(x,y, test_size=0.2, random_state=23, stratify=y)

xtrain


from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()
knn.fit(xtrain, ytrain)
predict=knn.predict(xtest)
predict

from sklearn.metrics import accuracy_score
accuracy_score(ytest, predict)

from sklearn.linear_model import LogisticRegression
lr=LogisticRegression()
lr.fit(xtrain, ytrain)
predict=lr.predict(xtest)
predict

from sklearn.metrics import accuracy_score
accuracy_score(ytest, predict)

from sklearn.neural_network import MLPClassifier
clf=MLPClassifier(solver='adam', hidden_layer_sizes=(5,2), random_state=2, max_iter=2000)
clf.fit(xtrain, ytrain)
predict2=clf.predict(xtest)
predict2

from sklearn.metrics import accuracy_score
accuracy_score(ytest, predict2)

from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score , classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler , OneHotEncoder
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)

models={
    "LogisticRegression":LogisticRegression(),
    "RandomForest":RandomForestClassifier(),
    "knn":KNeighborsClassifier(),
    "GradientBoosting":GradientBoostingClassifier(),
    "SVM":SVC()

}
results = {}

for name, model in models.items():
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('model', model)
    ])

    pipe.fit(x_train , y_train)
    y_pred = pipe.predict(x_test)
    acc= accuracy_score(y_test, y_pred)
    results[name] = acc

    print(f"{name} Accuracy: {acc:4f}")
    print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
plt.bar(results.keys(), results.values(),color='skyblue')
plt.ylabel('Accuracy Score')
plt.title('Model Accuracy Comparison')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)

models = {
    "LogisticRegression": LogisticRegression(max_iter=1000),
    "RandomForest": RandomForestClassifier(),
    "knn": KNeighborsClassifier(),
    "GradientBoosting": GradientBoostingClassifier(),
    "SVM": SVC()
}

results={}

for name, model in models.items():
    model.fit(x_train, y_train)
    preds = model.predict(x_test)
    acc = accuracy_score(y_test, preds)
    results[name] = acc

    print(f"{name} Accuracy: {acc:4f}")

best_model_name= max(results , key=results.get)
best=model = models[best_model_name]
print(f"\n Best model : {best_model_name} with accuracy {results[best_model_name]: 4f}")

joblib.dump(best, 'best_model.pkl')
print("Best model saved as best_model.pkl")


!pip install streamlit pyngrok

!ngrok authtoken 30YB6uJnSxTFTgPOfLh6Nrv9zHj_41RrKqwdXg7H8eTjd2ax8

import os
import threading

def run_streamlit():
   os.system("streamlit run app.py --server.port 8501")

thread = threading.Thread(target=run_streamlit)
thread.start()

pip install streamlit pandas scikit-learn

import joblib

joblib.dump(scaler, 'scaler.pkl') # Save the updated scaler

%%writefile app.py
import streamlit as st
import pandas as pd
import joblib
from sklearn.preprocessing import LabelEncoder, MinMaxScaler

# Load the trained model and scaler
model = joblib.load("best_model.pkl")
scaler = joblib.load("scaler.pkl")

st.set_page_config(page_title="Employee Salary Classification", page_icon="ðŸ’¼", layout="centered")

st.title("ðŸ’¼ Employee Salary Classification App")
st.markdown("Predict whether an employee earns >50K or â‰¤50K based on input features.")

# Load and preprocess the original data to fit encoders
try:
    original_data = pd.read_csv("/content/adult 3.csv")
    # Perform the same preprocessing steps as in the notebook
    original_data.occupation = original_data.occupation.replace({"?": "Private"})
    original_data.workclass = original_data.workclass.replace({"?": "Others"})
    original_data = original_data[original_data['workclass']!='Without-pay']
    original_data = original_data[original_data['workclass']!='Never-worked']
    original_data = original_data[original_data['education']!='1st-4th']
    original_data = original_data[original_data['education']!='Preschool']
    original_data = original_data[original_data['education']!='5th-6th']
    original_data.drop(columns=['education', 'fnlwgt'], inplace=True)
    original_data = original_data[(original_data['age']<=75)&(original_data['age']>=17)]

    categorical_cols = ['workclass', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country']
    encoders = {}
    for col in categorical_cols:
        encoders[col] = LabelEncoder()
        # Fit the encoder on the preprocessed original data
        encoders[col].fit(original_data[col])

except Exception as e:
    st.error(f"Error loading or preprocessing original data: {e}")
    st.stop()


# Sidebar inputs (these must match your training feature columns)
st.sidebar.header("Input Employee Details")

age = st.sidebar.slider("Age", 17, 75, 30)
workclass = st.sidebar.selectbox("Workclass", original_data['workclass'].unique())
educational_num = st.sidebar.slider("Educational Num", 1, 16, 9)
marital_status = st.sidebar.selectbox("Marital Status", original_data['marital-status'].unique())
occupation = st.sidebar.selectbox("Job Role", original_data['occupation'].unique())
relationship = st.sidebar.selectbox("Relationship", original_data['relationship'].unique())
race = st.sidebar.selectbox("Race", original_data['race'].unique())
gender = st.sidebar.selectbox("Gender", original_data['gender'].unique())
capital_gain = st.sidebar.slider("Capital Gain", 0, 100000, 0)
capital_loss = st.sidebar.slider("Capital Loss", 0, 100000, 0)
hours_per_week = st.sidebar.slider("Hours per week", 1, 99, 40)
native_country = st.sidebar.selectbox("Native Country", original_data['native-country'].unique())


# Build input DataFrame
input_df = pd.DataFrame({
    'age': [age],
    'workclass': [workclass],
    'educational-num': [educational_num],
    'marital-status': [marital_status],
    'occupation': [occupation],
    'relationship': [relationship],
    'race': [race],
    'gender': [gender],
    'capital-gain': [capital_gain],
    'capital-loss': [capital_loss],
    'hours-per-week': [hours_per_week],
    'native-country': [native_country],
})

# Convert categorical features to numerical using the fitted encoders
categorical_cols = ['workclass', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country']
for col in categorical_cols:
    # Ensure the value is in the encoder's known classes, handle unseen values if necessary
    if input_df[col][0] not in encoders[col].classes_:
        st.warning(f"Input value '{input_df[col][0]}' for '{col}' is not in the training data categories. Mapping to the first class.")
        # You might want to handle unseen values differently, e.g., map to a default or raise an error
        # For now, we'll proceed, but the transform might raise an error if unseen values are not handled by the encoder
    input_df[col] = encoders[col].transform(input_df[col])


# Scale the entire DataFrame using the same MinMaxScaler used in training
input_df_scaled = scaler.transform(input_df)


st.write("### ðŸ”Ž Input Data")
st.write(pd.DataFrame(input_df_scaled, columns=input_df.columns)) # Display as DataFrame after scaling with original column names

# Predict button
if st.button("Predict Salary Class"):
    prediction = model.predict(input_df_scaled)
    st.success(f"âœ… Prediction: {prediction[0]}")

# Batch prediction
st.markdown("---")
st.markdown("#### ðŸ“‚ Batch Prediction")
uploaded_file = st.file_uploader("Upload a CSV file for batch prediction", type="csv")

if uploaded_file is not None:
    batch_data = pd.read_csv(uploaded_file)

    # Preprocess batch data similar to training data
    # 1. Handle '?' values
    batch_data.occupation = batch_data.occupation.replace({"?": "Private"})
    batch_data.workclass = batch_data.workclass.replace({"?": "Others"})
    # 2. Drop irrelevant rows
    batch_data=batch_data[batch_data['workclass']!='Without-pay']
    batch_data=batch_data[batch_data['workclass']!='Never-worked']
    batch_data=batch_data[batch_data['education']!='1st-4th']
    batch_data=batch_data[batch_data['education']!='Preschool']
    batch_data=batch_data[batch_data['education']!='5th-6th']
    # 3. Drop columns
    batch_data.drop(columns=['education'], inplace=True)
    batch_data.drop(columns=['fnlwgt'], inplace=True)
    # 4. Handle age outliers
    batch_data=batch_data[(batch_data['age']<=75)&(batch_data['age']>=17)]

    # 5. Label encode categorical features using the same encoders
    for col in categorical_cols:
         if col in encoders: # Check if encoder exists
             # Handle unseen values in batch data if necessary before transforming
             batch_data[col] = batch_data[col].apply(lambda x: x if x in encoders[col].classes_ else encoders[col].classes_[0]) # Simple handling: map unseen to the first class
             batch_data[col] = encoders[col].transform(batch_data[col])
         else:
             st.error(f"No encoder found for column in batch data: {col}")
             st.stop()


    # 6. Scale the entire batch DataFrame using the same scaler
    batch_data_scaled = scaler.transform(batch_data)


    st.write("Uploaded data preview:", batch_data.head())
    batch_preds = model.predict(batch_data_scaled)
    batch_data['PredictedClass'] = batch_preds
    st.write("âœ… Predictions:")
    st.write(batch_data.head())
    csv = batch_data.to_csv(index=False).encode('utf-8')
    st.download_button("Download Predictions CSV", csv, file_name='predicted_classes.csv', mime='text/csv')

from pyngrok import ngrok
import time
import os

# Kill any existing ngrok tunnels
ngrok.kill()

# Wait a few seconds to ensure the process is terminated
time.sleep(5)

# Start a new ngrok tunnel for the Streamlit app on port 8501
try:
    public_url = ngrok.connect(8501)
    print('Your streamlit app is live here:', public_url)
except Exception as e:
    print(f"An error occurred: {e}")
    print("Please check your ngrok authtoken and ensure no other ngrok processes are running.")

